{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation pipeline\n",
    "- Load file to prepare\n",
    "- Clean the file\n",
    "- Create an index\n",
    "- Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import unicodedata\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_complex(file):\n",
    "    # Evaluates whether the file structure is simple or complex\n",
    "    # More about the Sefaria text structures:\n",
    "    # https://developers.sefaria.org/docs/the-index-schema\n",
    "    # Returns True if the structure is complexe\n",
    "\n",
    "    # open the json file and load the content\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # Check if 'text' key exists and is a dictionary\n",
    "    if 'text' in data and isinstance(data['text'], dict):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_simple_structure_and_show_unique_chars(filename):\n",
    "    '''\n",
    "    This function reads a simple structured file and returns a set of unique characters in the file.\n",
    "    The aim is to quickly indentify characters that should be removed from the text.\n",
    "    '''\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    character_set = set()\n",
    "\n",
    "    for chapter in data[\"text\"]:\n",
    "        for verse in chapter:\n",
    "            if isinstance(verse, str):\n",
    "                character_set.update(verse)\n",
    "            elif isinstance(verse, list):\n",
    "                for sub_verse in verse:\n",
    "                    character_set.update(sub_verse)\n",
    "\n",
    "    return character_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_complex_structure_and_show_unique_chars(filename):\n",
    "\n",
    "    '''\n",
    "    This function reads a complex structured file and returns a set of unique characters in the file.\n",
    "    The aim is to quickly indentify characters that should be removed from the text.\n",
    "    '''\n",
    "\n",
    "    def browse_and_show_unique_chars(nodes, level=0, character_set=None):\n",
    "        '''\n",
    "        recursively browse the nodes and return a set of unique characters in the file.\n",
    "        '''\n",
    "\n",
    "        if character_set is None:\n",
    "            character_set = set()\n",
    "\n",
    "        for value in nodes.values():\n",
    "            # If the node value is a list, add the unique characters of each list item to the set\n",
    "            if isinstance(value, list):\n",
    "                for elem in value:\n",
    "                    # If the element is a list, iterate through its items and add their unique characters to the set\n",
    "                    if isinstance(elem, list):\n",
    "                        for sub_elem in elem:\n",
    "                            if isinstance(sub_elem, list):\n",
    "                                for sub_sub_elem in sub_elem:\n",
    "                                    character_set.update(sub_sub_elem)                         \n",
    "                            else:\n",
    "                                character_set.update(sub_elem)\n",
    "                    else:\n",
    "                        character_set.update(elem)\n",
    "\n",
    "            # If the node value is a dictionary, recursively call the browse_nodes function on the dictionary.\n",
    "            elif isinstance(value, dict):\n",
    "                browse_and_show_unique_chars(value, level + 1, character_set)\n",
    "\n",
    "        return character_set\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    nodes = data['text']\n",
    "\n",
    "    return browse_and_show_unique_chars(nodes, level=0, character_set=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_file_and_show_unique_chars(filename):\n",
    "    '''\n",
    "    Identify whether the structure of the file is simple or complex,\n",
    "    and call the appropriate function to return the unique characters in the file.\n",
    "    '''\n",
    "    if is_complex(filename):\n",
    "        return browse_complex_structure_and_show_unique_chars(filename)\n",
    "    else:\n",
    "        return browse_simple_structure_and_show_unique_chars(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_simple_structured_files(filename):\n",
    "    '''\n",
    "    Create a function to browse simple structures files and create index entries.\n",
    "    With the index, we can then quickly identify the reference of a text chunk,\n",
    "    knowing the start and end character position of the text chunk in the book.\n",
    "    '''\n",
    "    # Load the file\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create a list to store index entries\n",
    "    index = {}\n",
    "    # add the title of the book to the index\n",
    "    index[\"title\"] = data[\"title\"]\n",
    "    index['categories'] = data['categories']\n",
    "    index['sectionNames'] = data['sectionNames']\n",
    "    index['lines'] = []\n",
    "\n",
    "    # Generate a unique identifier for each of the smallest text unit,\n",
    "    # E.g. verses.\n",
    "    def generate_uid(title, number):\n",
    "        return f\"{title.replace(' ', '_')}_{number}\"\n",
    "\n",
    "    # Counts the chars, and fills the index with start and end char position\n",
    "    # Hebrew diacritics are removed from the count.\n",
    "    def browse_text_and_count_verses_chars(text):\n",
    "        def remove_hebrew_diacritics(text):\n",
    "            normalized_text = unicodedata.normalize('NFKD', text)\n",
    "            return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "        # Variable to track unique identification number\n",
    "        uid = 1\n",
    "        # Initialize character counter\n",
    "        verse_start_char = 1\n",
    "        # Intialize chapter counter\n",
    "        chapter_number = 1\n",
    "\n",
    "        for chapter in data['text']:\n",
    "            # Initialize verse counter\n",
    "            verse_number = 1\n",
    "\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    verse = remove_hebrew_diacritics(verse)\n",
    "                    verse_end_char = verse_start_char + len(verse) - 1\n",
    "\n",
    "                    # Add verse to index with UID\n",
    "                    index['lines'].append({\n",
    "                        \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                        \"chapter_number\": chapter_number,\n",
    "                        \"verse_number\": verse_number,\n",
    "                        \"start_char\": verse_start_char,\n",
    "                        \"end_char\": verse_end_char,\n",
    "                        \"length\": len(verse),\n",
    "                        \"text\": verse\n",
    "                    })\n",
    "\n",
    "                    # Increment UID for next verse\n",
    "                    uid += 1\n",
    "\n",
    "                    # Increment verse number for next verse\n",
    "                    verse_number += 1\n",
    "\n",
    "                    # Update character counter for next verse\n",
    "                    verse_start_char = verse_end_char + 1\n",
    "                elif isinstance(verse, list):\n",
    "\n",
    "                    # Initialize subverse counter\n",
    "                    sub_verse_number = 1\n",
    "\n",
    "                    for sub_verse in verse:\n",
    "                        sub_verse = remove_hebrew_diacritics(sub_verse)\n",
    "                        sub_verse_end_char = verse_start_char + len(sub_verse) - 1\n",
    "\n",
    "                        # Add verse to index with UID\n",
    "                        index['lines'].append({\n",
    "                            \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                            \"chapter_number\": chapter_number,\n",
    "                            \"verse_number\": verse_number,\n",
    "                            \"sub_verse_number\": sub_verse_number,\n",
    "                            \"start_char\": verse_start_char,\n",
    "                            \"end_char\": sub_verse_end_char,\n",
    "                            \"length\": len(sub_verse),\n",
    "                            \"text\": sub_verse\n",
    "                        })\n",
    "\n",
    "                        # Increment UID for next verse\n",
    "                        uid += 1\n",
    "\n",
    "                        # Increment verse number for next verse\n",
    "                        sub_verse_number += 1\n",
    "\n",
    "                        # Update character counter for next verse\n",
    "                        verse_start_char = sub_verse_end_char + 1\n",
    "                    verse_number += 1\n",
    "            chapter_number += 1\n",
    "    \n",
    "    # Load the nodes\n",
    "    text = data['text']\n",
    "    browse_text_and_count_verses_chars(text)\n",
    "\n",
    "    # Create a new directory named \"indexes\" if it doesn't exist\n",
    "    index_dir = \"indexes\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    # Save the index to a file, in the \"indexes\" directory\n",
    "    title = index[\"title\"]\n",
    "    index_filename = os.path.join(index_dir, \"index_\" + title.replace(' ', '_') + \".json\")\n",
    "    with open(index_filename, \"w\") as f:\n",
    "        json.dump(index, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_complex_structured_files(filename):\n",
    "    '''\n",
    "    Create a function to browse complex structured files and create index entries.\n",
    "    With the index, we can then quickly identify the reference of a text chunk,\n",
    "    knowing the start and end character position of the text chunk in the book.\n",
    "    '''\n",
    "\n",
    "    # Load the file\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create a list to store index entries\n",
    "    index = {}\n",
    "    # add the title of the book to the index\n",
    "    index[\"title\"] = data[\"title\"]\n",
    "    index['categories'] = data['categories']\n",
    "    index['lines'] = []\n",
    "\n",
    "    def generate_uid(title, number):\n",
    "        return f\"{title.replace(' ', '_')}_{number}\"\n",
    "\n",
    "    def browse_nodes_and_count_verses_chars(nodes, parent_keys=[], level=0):\n",
    "        def remove_hebrew_diacritics(text):\n",
    "            normalized_text = unicodedata.normalize('NFKD', text)\n",
    "            return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "        # Variable to track unique identification number\n",
    "        uid = 1\n",
    "\n",
    "        # Initialize character counter\n",
    "        verse_start_char = 1\n",
    "\n",
    "        item_lvl_1_number, item_lvl_2_number, item_lvl_3_number = 0, 0, 0\n",
    "\n",
    "\n",
    "        for key, value in nodes.items():\n",
    "            # Add the current key to the list of parent keys\n",
    "            current_keys = parent_keys + [key]\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                # If the node value is a list, check if it contains strings or lists\n",
    "\n",
    "                # Initialize verse counter\n",
    "                item_lvl_1_number = 1\n",
    "\n",
    "                for item_lvl_1 in value:\n",
    "                # 1st lvl: If the item is a string, treat it as a single verse\n",
    "                    if isinstance(item_lvl_1, str):\n",
    "                        # remove hebrew vowels\n",
    "                        item_lvl_1 = remove_hebrew_diacritics(item_lvl_1)\n",
    "                        verse_end_char = verse_start_char + len(item_lvl_1) - 1\n",
    "\n",
    "                        # Add verse to index with UID\n",
    "                        index['lines'].append({\n",
    "                            \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                            \"parent_titles\": current_keys,\n",
    "                            \"item_lvl_1_number\": item_lvl_1_number,\n",
    "                            \"start_char\": verse_start_char,\n",
    "                            \"end_char\": verse_end_char,\n",
    "                            \"length\": len(item_lvl_1),\n",
    "                            \"text\": item_lvl_1\n",
    "                        })\n",
    "\n",
    "                        # Increment UID for next verse\n",
    "                        uid += 1\n",
    "\n",
    "\n",
    "                        # Update character counter for next verse\n",
    "                        verse_start_char = verse_end_char + 1\n",
    "\n",
    "\n",
    "                    # 1st lvl: if the item in value is a list, treat it as a list\n",
    "                    if isinstance(item_lvl_1, list):\n",
    "                        # Initialize verse counter\n",
    "                        item_lvl_2_number = 1\n",
    "                        for item_lvl_2 in item_lvl_1:\n",
    "                            # 2nd lvl:\n",
    "                            #  if item is a string, treat it as a single verse\n",
    "                            if isinstance(item_lvl_2, str):\n",
    "\n",
    "                                # remove hebrew vowels\n",
    "                                item_lvl_2 = remove_hebrew_diacritics(item_lvl_2)\n",
    "                                verse_end_char = verse_start_char + len(item_lvl_2) - 1\n",
    "\n",
    "                                # Add verse to index with UID\n",
    "                                index['lines'].append({\n",
    "                                    \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                                    \"parent_titles\": current_keys,\n",
    "                                    \"item_lvl_1_number\": item_lvl_1_number,\n",
    "                                    \"item_lvl_2_number\": item_lvl_2_number,\n",
    "                                    \"start_char\": verse_start_char,\n",
    "                                    \"end_char\": verse_end_char,\n",
    "                                    \"length\": len(item_lvl_2),\n",
    "                                    \"text\": item_lvl_2\n",
    "                                })\n",
    "\n",
    "                                # Increment UID for next verse\n",
    "                                uid += 1\n",
    "\n",
    "                                # Increment verse number for next verse\n",
    "                                item_lvl_2_number += 1\n",
    "\n",
    "                                # Update character counter for next verse\n",
    "                                verse_start_char = verse_end_char + 1\n",
    "\n",
    "                            # 2nd lvl: if the item is a list\n",
    "                            elif isinstance(item_lvl_2, list):\n",
    "                                # Initialize subverse counter\n",
    "                                item_lvl_3_number = 1\n",
    "                                for item_lvl_3 in item_lvl_2:\n",
    "                                \n",
    "                                    # 3rd lvl: if the sub_verse is a string\n",
    "                                    if isinstance(item_lvl_3, str):\n",
    "                                        item_lvl_3 = remove_hebrew_diacritics(item_lvl_3)\n",
    "                                        verse_end_char = verse_start_char + len(item_lvl_3) - 1\n",
    "\n",
    "                                        # Add verse to index with UID\n",
    "                                        index['lines'].append({\n",
    "                                        \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                                        \"parent_titles\": current_keys,\n",
    "                                        \"item_lvl_1_number\": item_lvl_1_number,\n",
    "                                        \"item_lvl_2_number\": item_lvl_2_number,\n",
    "                                        \"item_lvl_3_number\": item_lvl_3_number,\n",
    "                                        \"start_char\": verse_start_char,\n",
    "                                        \"end_char\": verse_end_char,\n",
    "                                        \"length\": len(item_lvl_3),\n",
    "                                        \"text\": item_lvl_3\n",
    "                                        })\n",
    "\n",
    "                                        # Increment UID for next verse\n",
    "                                        uid += 1\n",
    "\n",
    "                                        # Increment verse number for next verse\n",
    "                                        item_lvl_3_number += 1\n",
    "\n",
    "                                        # Update character counter for next verse\n",
    "                                        verse_start_char = verse_end_char + 1\n",
    "\n",
    "                                    # 3rd lvl: if the sub_verse is a list\n",
    "                                    if isinstance(item_lvl_3, list):\n",
    "                                        print(\"nested verses detected and not handled !\")\n",
    "\n",
    "                                item_lvl_2_number += 1\n",
    "                        # Increment verse number for next verse\n",
    "                        item_lvl_1_number += 1\n",
    "\n",
    "\n",
    "            # If the node value is a dictionary, recursively call the browse_nodes_and_count_verses_chars function on the dictionary.\n",
    "            elif isinstance(value, dict):\n",
    "                browse_nodes_and_count_verses_chars(value, parent_keys=current_keys, level=level + 1)\n",
    "\n",
    "    # Load the nodes\n",
    "    nodes = data['text']\n",
    "    browse_nodes_and_count_verses_chars(nodes, level=0)\n",
    "\n",
    "    # Create a new directory named \"indexes\" if it doesn't exist\n",
    "    index_dir = \"indexes\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    # Save the index to a file, in the \"indexes\" directory\n",
    "    title = index[\"title\"]\n",
    "    index_filename = os.path.join(index_dir, \"index_\" + title.replace(' ', '_') + \".json\")\n",
    "    with open(index_filename, \"w\") as f:\n",
    "        json.dump(index, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(filename):\n",
    "    '''\n",
    "    Create an index from a structured file.\n",
    "    The index will contain the start and end character position of each text chunk in the book.\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "    \n",
    "    if is_complex(filename) == True:\n",
    "        create_index_from_complex_structured_files(filename)\n",
    "    \n",
    "    else:\n",
    "        create_index_from_simple_structured_files(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_and_clean(filename):\n",
    "\n",
    "    '''\n",
    "    Remove unwanted characters from the text (e.g. numbers, special unicodes, html tags, etc.)\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "\n",
    "    def remove_angle_brackets(text):\n",
    "        return re.sub(r'\\<.*?\\>', '', text)\n",
    "\n",
    "    def remove_special_unicodes(text):\n",
    "        chars_to_remove = {'\\n', '\\u2003', '\\u200d', '\\u200e', '\\u202c','\\u2009', '\\xa0' ,'\\xad'}\n",
    "        for char in chars_to_remove:\n",
    "            text = text.replace(char, '')\n",
    "        return text\n",
    "\n",
    "    def remove_numbers(text):\n",
    "        return ''.join(char for char in text if not char.isdigit())\n",
    "    \n",
    "    def clean_html(text):\n",
    "        if text.strip():  # Checks if text is not empty after removing spaces\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            \n",
    "            # Specifically deletes <b> and </b> tags\n",
    "            for bold_tag in soup.find_all(['b', 'strong']):\n",
    "                bold_tag.unwrap()\n",
    "            \n",
    "            return soup.get_text()\n",
    "        else:\n",
    "            return text  # Returns text unchanged if empty\n",
    "\n",
    "    # we try to catch and display the MarkupResemblesLocatorWarning warning\n",
    "    # import warnings\n",
    "\n",
    "    # def clean_html(text):\n",
    "    #     with warnings.catch_warnings(record=True) as w:\n",
    "    #         warnings.simplefilter(\"always\")  # Capture tous les avertissements\n",
    "    #         if text.strip():\n",
    "    #             text_to_parse = text  # Stocker le texte avant de l'analyser\n",
    "    #             soup = BeautifulSoup(text, 'html.parser')\n",
    "    #             text_without_html = soup.get_text()\n",
    "    #             # Parcourir tous les avertissements capturés\n",
    "    #             for warning in w:\n",
    "    #                 if \"MarkupResemblesLocatorWarning\" in str(warning.message):\n",
    "    #                     print(\"MarkupResemblesLocatorWarning captured while parsing the following text:\")\n",
    "    #                     print(text_to_parse)\n",
    "    #             return text\n",
    "    #         else:\n",
    "    #             return text\n",
    "\n",
    "    \n",
    "    def remove_braces(text):\n",
    "        return re.sub(r'\\{.*?\\}', '', text)\n",
    "   \n",
    "\n",
    "    def clean_text(text):\n",
    "        # text = remove_angle_brackets(text)\n",
    "        text = clean_html(text)\n",
    "        text = remove_special_unicodes(text)\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_braces(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "\n",
    "    if isinstance(data['text'], list):\n",
    "\n",
    "        # Browse and clean verses directly in the data structure\n",
    "        for chapter in data[\"text\"]:\n",
    "            if isinstance(chapter, str):\n",
    "                chapter = clean_text(chapter)\n",
    "            else:\n",
    "                for i, verse in enumerate(chapter):\n",
    "                    if isinstance(verse, str):\n",
    "                        chapter[i] = clean_text(verse)\n",
    "                    elif isinstance(verse, list):\n",
    "                        for j, sub_verse in enumerate(verse):\n",
    "                            verse[j] = clean_text(sub_verse)        \n",
    "    \n",
    "    \n",
    "    elif isinstance(data['text'], dict):\n",
    "        \n",
    "        def browse_clean_complex(nodes, level=0):\n",
    "            for key, value in nodes.items():\n",
    "                if isinstance(value, list):\n",
    "                    for i, elem in enumerate(value):\n",
    "                        if isinstance(elem, list):\n",
    "                            for j, sub_elem in enumerate(elem):\n",
    "                                if isinstance(sub_elem, list):\n",
    "                                    for k, sub_sub_elem in enumerate(sub_elem):\n",
    "                                        nodes[key][i][j][k] = clean_text(sub_sub_elem)\n",
    "                                else:\n",
    "                                    nodes[key][i][j] = clean_text(sub_elem)\n",
    "                        else:\n",
    "                            nodes[key][i] = clean_text(elem)\n",
    "                elif isinstance(value, dict):\n",
    "                    browse_clean_complex(value, level + 1)\n",
    "            return nodes\n",
    "\n",
    "        \n",
    "        # Load the nodes\n",
    "        nodes = data['text']\n",
    "        browse_clean_complex(nodes, level=0)\n",
    "\n",
    "    # Create a new directory named \"cleaned\" if it doesn't exist\n",
    "    if not os.path.exists('cleaned'):\n",
    "        os.makedirs('cleaned')\n",
    "\n",
    "    # Define the cleaned filename with spaces replaced by underscores\n",
    "    base_filename = os.path.basename(filename)\n",
    "    cleaned_filename = os.path.join('cleaned', base_filename.replace(' ', '_').split('.')[0] + \"_clean.json\")\n",
    "    \n",
    "    \n",
    "    # Save the cleaned file in the \"cleaned\" directory\n",
    "    with open(cleaned_filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_verses(filename):\n",
    "    '''\n",
    "    Concatenate the verses of a book into a single text file.\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    concatenated_text = \"\"\n",
    "\n",
    "    def remove_hebrew_diacritics(text):\n",
    "        normalized_text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "    if isinstance(data['text'], list):\n",
    "        # Browse and clean verses directly in the data structure\n",
    "        for chapter in data[\"text\"]:\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    concatenated_text += verse\n",
    "                elif isinstance(verse, list):\n",
    "                    for sub_verse in verse:\n",
    "                        if isinstance(sub_verse, str):\n",
    "                            concatenated_text += sub_verse\n",
    "\n",
    "    elif isinstance(data['text'], dict):\n",
    "\n",
    "        def browse_concat_complex(nodes):\n",
    "            text = \"\"\n",
    "            for value in nodes.values():\n",
    "                if isinstance(value, list):\n",
    "                    for elem in value:\n",
    "                        if isinstance(elem, str):\n",
    "                            text += elem\n",
    "                        elif isinstance(elem, list):\n",
    "                            for sub_elem in elem:\n",
    "                                if isinstance(sub_elem, str):\n",
    "                                    text += sub_elem\n",
    "                                elif isinstance(sub_elem, list):\n",
    "                                    for sub_sub_elem in sub_elem:\n",
    "                                        if isinstance(sub_sub_elem, str):\n",
    "                                            text += sub_sub_elem\n",
    "                                        elif isinstance(sub_sub_elem, list):\n",
    "                                            print(\"Nested verses detected and not handled!\")\n",
    "                elif isinstance(value, dict):\n",
    "                    text += browse_concat_complex(value)\n",
    "            return text\n",
    "\n",
    "        # Load the nodes\n",
    "        concatenated_text = browse_concat_complex(data['text'])\n",
    "\n",
    "    # Create a new directory named \"concatenated\" if it doesn't exist\n",
    "    concatenated_dir = 'concatenated'\n",
    "    if not os.path.exists(concatenated_dir):\n",
    "        os.makedirs(concatenated_dir)\n",
    "\n",
    "    # Save the concatenated text to a file, in the \"concatenated\" directory\n",
    "    concatenated_filename = os.path.join(concatenated_dir, os.path.splitext(os.path.basename(filename))[0] + \"_concatenated.txt\")\n",
    "    with open(concatenated_filename, \"w\") as f:\n",
    "        f.write(remove_hebrew_diacritics(concatenated_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process was completed without error.\n"
     ]
    }
   ],
   "source": [
    "# For all the files in the directory, browse, clean and concatenate the verses\n",
    "import os\n",
    "path = \"../../Corpuses_from_Sefaria/Bavli\"\n",
    "\n",
    "# Variable to track whether an error has occurred\n",
    "error_occurred = False\n",
    "\n",
    "# Clean the texts in the directory\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            browse_and_clean(os.path.join(path, filename))\n",
    "        except Exception as e:\n",
    "            error_occurred = True\n",
    "            print(f\"An error occurred while processing file {filename}: {str(e)}\")\n",
    "\n",
    "# create an index\n",
    "for filename in os.listdir(\"cleaned\"):\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            create_index(os.path.join(\"cleaned\", filename))\n",
    "        except Exception as e:\n",
    "            error_occurred = True\n",
    "            print(f\"An error occurred while creating index for file {filename}: {str(e)}\")\n",
    "\n",
    "# Concatenate the verses in the 'cleaned' directory\n",
    "for filename in os.listdir(\"cleaned\"):\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            concatenate_verses(os.path.join(\"cleaned\", filename))\n",
    "        except Exception as e:\n",
    "            error_occurred = True\n",
    "            print(f\"An error occurred while concatenating verses for file {filename}: {str(e)}\")\n",
    "\n",
    "# Afficher un message indiquant que le processus s'est terminé sans erreur\n",
    "if not error_occurred:\n",
    "    print(\"The process was completed without error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# path = \"../../Corpuses_from_Sefaria/Liturgy\"\n",
    "\n",
    "# # Variable to track whether an error has occurred\n",
    "# error_occurred = False\n",
    "\n",
    "# # Clean the texts in the directory\n",
    "# for filename in os.listdir(path):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         browse_and_clean(os.path.join(path, filename))\n",
    "\n",
    "# # create an index\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         create_index(os.path.join(\"cleaned\", filename))\n",
    "\n",
    "# # Concatenate the verses in the 'cleaned' directory\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         concatenate_verses(os.path.join(\"cleaned\", filename))\n",
    "\n",
    "# # Afficher un message indiquant que le processus s'est terminé sans erreur\n",
    "# if not error_occurred:\n",
    "#     print(\"The process was completed without error.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
