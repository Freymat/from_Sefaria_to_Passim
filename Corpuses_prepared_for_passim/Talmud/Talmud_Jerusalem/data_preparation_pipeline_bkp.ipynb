{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation pipeline\n",
    "- Load file to prepare\n",
    "- Clean the file\n",
    "- Create an index\n",
    "- Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import unicodedata\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_and_show_unique_chars(filename):\n",
    "    '''\n",
    "    This function reads a simple structured file and returns a set of unique characters in the file.\n",
    "    The aim is to quickly indentify characters that should be removed from the text.\n",
    "    '''\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        all_books = json.load(f)\n",
    "    \n",
    "    character_set = set()\n",
    "\n",
    "    for book in all_books:\n",
    "        for chapter in book[\"text\"]:\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    character_set.update(verse)\n",
    "                elif isinstance(verse, list):\n",
    "                    for sub_verse in verse:\n",
    "                        character_set.update(sub_verse)\n",
    "\n",
    "    return character_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " 'C',\n",
       " 'G',\n",
       " 'H',\n",
       " 'J',\n",
       " 'L',\n",
       " 'N',\n",
       " 'P',\n",
       " 'T',\n",
       " 'V',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'y',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '·',\n",
       " '̇',\n",
       " '̈',\n",
       " '͏',\n",
       " '֑',\n",
       " '֒',\n",
       " '֓',\n",
       " '֔',\n",
       " '֕',\n",
       " '֖',\n",
       " '֗',\n",
       " '֘',\n",
       " '֙',\n",
       " '֚',\n",
       " '֛',\n",
       " '֜',\n",
       " '֝',\n",
       " '֞',\n",
       " '֠',\n",
       " '֡',\n",
       " '֢',\n",
       " '֣',\n",
       " '֤',\n",
       " '֥',\n",
       " '֦',\n",
       " '֧',\n",
       " '֨',\n",
       " '֩',\n",
       " '֪',\n",
       " '֫',\n",
       " '֬',\n",
       " '֭',\n",
       " '֮',\n",
       " 'ְ',\n",
       " 'ֱ',\n",
       " 'ֲ',\n",
       " 'ֳ',\n",
       " 'ִ',\n",
       " 'ֵ',\n",
       " 'ֶ',\n",
       " 'ַ',\n",
       " 'ָ',\n",
       " 'ֹ',\n",
       " 'ֻ',\n",
       " 'ּ',\n",
       " 'ֽ',\n",
       " '־',\n",
       " 'ֿ',\n",
       " '׀',\n",
       " 'ׁ',\n",
       " 'ׂ',\n",
       " '׃',\n",
       " 'ׄ',\n",
       " 'ׇ',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'ף',\n",
       " 'פ',\n",
       " 'ץ',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת',\n",
       " 'װ',\n",
       " '׳',\n",
       " '״',\n",
       " '\\u2009',\n",
       " '\\u200c',\n",
       " '\\u200d',\n",
       " '\\u200e',\n",
       " '\\u200f',\n",
       " '‘',\n",
       " '…',\n",
       " '\\u202c',\n",
       " 'ﬞ',\n",
       " 'תּ'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'Jerusalem_Talmud_complete.json'\n",
    "\n",
    "browse_and_show_unique_chars(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(filename):\n",
    "    '''\n",
    "    Create a function to browse simple structures files and create index entries.\n",
    "    With the index, we can then quickly identify the reference of a text chunk,\n",
    "    knowing the start and end character position of the text chunk in the book.\n",
    "    '''\n",
    "    # Load the file\n",
    "    with open(filename, \"r\") as f:\n",
    "        all_books = json.load(f)\n",
    "\n",
    "    # Create a list to store index entries\n",
    "    index = {}\n",
    "    # add the title of the book to the index\n",
    "    index[\"title\"] = all_books[\"title\"]\n",
    "    index['categories'] = all_books['categories']\n",
    "    index['sectionNames'] = all_books['sectionNames']\n",
    "    index['lines'] = []\n",
    "\n",
    "    # Generate a unique identifier for each of the smallest text unit,\n",
    "    # E.g. verses.\n",
    "    def generate_uid(title, number):\n",
    "        return f\"{title.replace(' ', '_')}_{number}\"\n",
    "\n",
    "    # Counts the chars, and fills the index with start and end char position\n",
    "    # Hebrew diacritics are removed from the count.\n",
    "    def browse_text_and_count_verses_chars(text):\n",
    "        def remove_hebrew_diacritics(text):\n",
    "            normalized_text = unicodedata.normalize('NFKD', text)\n",
    "            return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "        # Variable to track unique identification number\n",
    "        uid = 1\n",
    "        # Initialize character counter\n",
    "        verse_start_char = 1\n",
    "        # Intialize chapter counter\n",
    "        chapter_number = 1\n",
    "\n",
    "        for chapter in all_books['text']:\n",
    "            # Initialize verse counter\n",
    "            verse_number = 1\n",
    "\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    verse = remove_hebrew_diacritics(verse)\n",
    "                    verse_end_char = verse_start_char + len(verse) - 1\n",
    "\n",
    "                    # Add verse to index with UID\n",
    "                    index['lines'].append({\n",
    "                        \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                        \"chapter_number\": chapter_number,\n",
    "                        \"verse_number\": verse_number,\n",
    "                        \"start_char\": verse_start_char,\n",
    "                        \"end_char\": verse_end_char,\n",
    "                        \"length\": len(verse),\n",
    "                        \"text\": verse\n",
    "                    })\n",
    "\n",
    "                    # Increment UID for next verse\n",
    "                    uid += 1\n",
    "\n",
    "                    # Increment verse number for next verse\n",
    "                    verse_number += 1\n",
    "\n",
    "                    # Update character counter for next verse\n",
    "                    verse_start_char = verse_end_char + 1\n",
    "                elif isinstance(verse, list):\n",
    "\n",
    "                    # Initialize subverse counter\n",
    "                    sub_verse_number = 1\n",
    "\n",
    "                    for sub_verse in verse:\n",
    "                        sub_verse = remove_hebrew_diacritics(sub_verse)\n",
    "                        sub_verse_end_char = verse_start_char + len(sub_verse) - 1\n",
    "\n",
    "                        # Add verse to index with UID\n",
    "                        index['lines'].append({\n",
    "                            \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                            \"chapter_number\": chapter_number,\n",
    "                            \"verse_number\": verse_number,\n",
    "                            \"sub_verse_number\": sub_verse_number,\n",
    "                            \"start_char\": verse_start_char,\n",
    "                            \"end_char\": sub_verse_end_char,\n",
    "                            \"length\": len(sub_verse),\n",
    "                            \"text\": sub_verse\n",
    "                        })\n",
    "\n",
    "                        # Increment UID for next verse\n",
    "                        uid += 1\n",
    "\n",
    "                        # Increment verse number for next verse\n",
    "                        sub_verse_number += 1\n",
    "\n",
    "                        # Update character counter for next verse\n",
    "                        verse_start_char = sub_verse_end_char + 1\n",
    "                    verse_number += 1\n",
    "            chapter_number += 1\n",
    "    \n",
    "    # Load the nodes\n",
    "    text = all_books['text']\n",
    "    browse_text_and_count_verses_chars(text)\n",
    "\n",
    "    # Create a new directory named \"indexes\" if it doesn't exist\n",
    "    index_dir = \"indexes\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    # Save the index to a file, in the \"indexes\" directory\n",
    "    title = index[\"title\"]\n",
    "    index_filename = os.path.join(index_dir, \"index_\" + title.replace(' ', '_') + \".json\")\n",
    "    with open(index_filename, \"w\") as f:\n",
    "        json.dump(index, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m, in \u001b[0;36mcreate_index\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# add the title of the book to the index\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mall_books\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_books[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msectionNames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_books[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msectionNames\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "create_index(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_and_clean(filename):\n",
    "\n",
    "    '''\n",
    "    Remove unwanted characters from the text (e.g. numbers, special unicodes, html tags, etc.)\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "\n",
    "    def remove_angle_brackets(text):\n",
    "        return re.sub(r'\\<.*?\\>', '', text)\n",
    "\n",
    "    def remove_special_unicodes(text):\n",
    "        chars_to_remove = {'\\n', '\\u2003', '\\u200d', '\\u200e', '\\u202c','\\u2009', '\\xa0' ,'\\xad'}\n",
    "        for char in chars_to_remove:\n",
    "            text = text.replace(char, '')\n",
    "        return text\n",
    "\n",
    "    def remove_numbers(text):\n",
    "        return ''.join(char for char in text if not char.isdigit())\n",
    "    \n",
    "    def clean_html(text):\n",
    "        if text.strip():  # Checks if text is not empty after removing spaces\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            \n",
    "            # Specifically deletes <b> and </b> tags\n",
    "            for bold_tag in soup.find_all(['b', 'strong']):\n",
    "                bold_tag.unwrap()\n",
    "            \n",
    "            return soup.get_text()\n",
    "        else:\n",
    "            return text  # Returns text unchanged if empty\n",
    "\n",
    "    # we try to catch and display the MarkupResemblesLocatorWarning warning\n",
    "    # import warnings\n",
    "\n",
    "    # def clean_html(text):\n",
    "    #     with warnings.catch_warnings(record=True) as w:\n",
    "    #         warnings.simplefilter(\"always\")  # Capture tous les avertissements\n",
    "    #         if text.strip():\n",
    "    #             text_to_parse = text  # Stocker le texte avant de l'analyser\n",
    "    #             soup = BeautifulSoup(text, 'html.parser')\n",
    "    #             text_without_html = soup.get_text()\n",
    "    #             # Parcourir tous les avertissements capturés\n",
    "    #             for warning in w:\n",
    "    #                 if \"MarkupResemblesLocatorWarning\" in str(warning.message):\n",
    "    #                     print(\"MarkupResemblesLocatorWarning captured while parsing the following text:\")\n",
    "    #                     print(text_to_parse)\n",
    "    #             return text\n",
    "    #         else:\n",
    "    #             return text\n",
    "    \n",
    "    def remove_braces(text):\n",
    "        return re.sub(r'\\{.*?\\}', '', text)\n",
    " \n",
    "    def clean_text(text):\n",
    "        # text = remove_angle_brackets(text)\n",
    "        text = clean_html(text)\n",
    "        text = remove_special_unicodes(text)\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_braces(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        all_books = json.load(f)\n",
    "\n",
    "    for book in all_books:\n",
    "\n",
    "        if isinstance(book['text'], list):\n",
    "\n",
    "            # Browse and clean verses directly in the data structure\n",
    "            for chapter in book[\"text\"]:\n",
    "                if isinstance(chapter, str):\n",
    "                    chapter = clean_text(chapter)\n",
    "                else:\n",
    "                    for i, verse in enumerate(chapter):\n",
    "                        if isinstance(verse, str):\n",
    "                            chapter[i] = clean_text(verse)\n",
    "                        elif isinstance(verse, list):\n",
    "                            for j, sub_verse in enumerate(verse):\n",
    "                                verse[j] = clean_text(sub_verse)    \n",
    " \n",
    "        \n",
    "\n",
    "\n",
    "    # Create a new directory named \"cleaned\" if it doesn't exist\n",
    "    if not os.path.exists('cleaned'):\n",
    "        os.makedirs('cleaned')\n",
    "\n",
    "    # Define the cleaned filename with spaces replaced by underscores\n",
    "    base_filename = os.path.basename(filename)\n",
    "    cleaned_filename = os.path.join('cleaned', base_filename.replace(' ', '_').split('.')[0] + \"_clean.json\")\n",
    "    \n",
    "    \n",
    "    # Save the cleaned file in the \"cleaned\" directory\n",
    "    with open(cleaned_filename, \"w\") as f:\n",
    "        json.dump(all_books, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Jerusalem_Talmud_complete.json'\n",
    "browse_and_clean(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'G',\n",
       " '[',\n",
       " ']',\n",
       " '|',\n",
       " '·',\n",
       " '̇',\n",
       " '̈',\n",
       " '͏',\n",
       " '֑',\n",
       " '֒',\n",
       " '֓',\n",
       " '֔',\n",
       " '֕',\n",
       " '֖',\n",
       " '֗',\n",
       " '֘',\n",
       " '֙',\n",
       " '֚',\n",
       " '֛',\n",
       " '֜',\n",
       " '֝',\n",
       " '֞',\n",
       " '֠',\n",
       " '֡',\n",
       " '֢',\n",
       " '֣',\n",
       " '֤',\n",
       " '֥',\n",
       " '֦',\n",
       " '֧',\n",
       " '֨',\n",
       " '֩',\n",
       " '֪',\n",
       " '֫',\n",
       " '֬',\n",
       " '֭',\n",
       " '֮',\n",
       " 'ְ',\n",
       " 'ֱ',\n",
       " 'ֲ',\n",
       " 'ֳ',\n",
       " 'ִ',\n",
       " 'ֵ',\n",
       " 'ֶ',\n",
       " 'ַ',\n",
       " 'ָ',\n",
       " 'ֹ',\n",
       " 'ֻ',\n",
       " 'ּ',\n",
       " 'ֽ',\n",
       " '־',\n",
       " 'ֿ',\n",
       " '׀',\n",
       " 'ׁ',\n",
       " 'ׂ',\n",
       " '׃',\n",
       " 'ׄ',\n",
       " 'ׇ',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'ף',\n",
       " 'פ',\n",
       " 'ץ',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת',\n",
       " 'װ',\n",
       " '׳',\n",
       " '״',\n",
       " '\\u200c',\n",
       " '\\u200f',\n",
       " '‘',\n",
       " '…',\n",
       " 'ﬞ',\n",
       " 'תּ'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'cleaned/Jerusalem_Talmud_complete_clean.json'\n",
    "browse_and_show_unique_chars(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_verses(filename):\n",
    "    '''\n",
    "    Concatenate the verses of a book into a single text file.\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    concatenated_text = \"\"\n",
    "\n",
    "    def remove_hebrew_diacritics(text):\n",
    "        normalized_text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "    if isinstance(data['text'], list):\n",
    "        # Browse and clean verses directly in the data structure\n",
    "        for chapter in data[\"text\"]:\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    concatenated_text += verse\n",
    "                elif isinstance(verse, list):\n",
    "                    for sub_verse in verse:\n",
    "                        if isinstance(sub_verse, str):\n",
    "                            concatenated_text += sub_verse\n",
    "\n",
    "    elif isinstance(data['text'], dict):\n",
    "\n",
    "        def browse_concat_complex(nodes):\n",
    "            text = \"\"\n",
    "            for value in nodes.values():\n",
    "                if isinstance(value, list):\n",
    "                    for elem in value:\n",
    "                        if isinstance(elem, str):\n",
    "                            text += elem\n",
    "                        elif isinstance(elem, list):\n",
    "                            for sub_elem in elem:\n",
    "                                if isinstance(sub_elem, str):\n",
    "                                    text += sub_elem\n",
    "                                elif isinstance(sub_elem, list):\n",
    "                                    for sub_sub_elem in sub_elem:\n",
    "                                        if isinstance(sub_sub_elem, str):\n",
    "                                            text += sub_sub_elem\n",
    "                                        elif isinstance(sub_sub_elem, list):\n",
    "                                            print(\"Nested verses detected and not handled!\")\n",
    "                elif isinstance(value, dict):\n",
    "                    text += browse_concat_complex(value)\n",
    "            return text\n",
    "\n",
    "        # Load the nodes\n",
    "        concatenated_text = browse_concat_complex(data['text'])\n",
    "\n",
    "    # Create a new directory named \"concatenated\" if it doesn't exist\n",
    "    concatenated_dir = 'concatenated'\n",
    "    if not os.path.exists(concatenated_dir):\n",
    "        os.makedirs(concatenated_dir)\n",
    "\n",
    "    # Save the concatenated text to a file, in the \"concatenated\" directory\n",
    "    concatenated_filename = os.path.join(concatenated_dir, os.path.splitext(os.path.basename(filename))[0] + \"_concatenated.txt\")\n",
    "    with open(concatenated_filename, \"w\") as f:\n",
    "        f.write(remove_hebrew_diacritics(concatenated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "def concatenate_verses(filename):\n",
    "    '''\n",
    "    Concatenate the verses of a book into a single text file.\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    concatenated_text = \"\"\n",
    "\n",
    "    def remove_hebrew_diacritics(text):\n",
    "        normalized_text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "    def extract_text_from_node(node):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if key == 'text':\n",
    "                    concatenated_texts.append(concatenate_from_text(value))\n",
    "                else:\n",
    "                    extract_text_from_node(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                extract_text_from_node(item)\n",
    "\n",
    "    def concatenate_from_text(text_data):\n",
    "        text = \"\"\n",
    "        if isinstance(text_data, list):\n",
    "            for element in text_data:\n",
    "                if isinstance(element, str):\n",
    "                    text += element\n",
    "                elif isinstance(element, list) or isinstance(element, dict):\n",
    "                    text += concatenate_from_text(element)\n",
    "        elif isinstance(text_data, str):\n",
    "            text += text_data\n",
    "        return text\n",
    "\n",
    "    concatenated_texts = []\n",
    "    extract_text_from_node(data)\n",
    "\n",
    "    # Concatenate all extracted texts into a single string\n",
    "    concatenated_text = \"\".join(concatenated_texts)\n",
    "\n",
    "    # Create a new directory named \"concatenated\" if it doesn't exist\n",
    "    concatenated_dir = 'concatenated'\n",
    "    if not os.path.exists(concatenated_dir):\n",
    "        os.makedirs(concatenated_dir)\n",
    "\n",
    "    # Save the concatenated text to a file, in the \"concatenated\" directory\n",
    "    concatenated_filename = os.path.join(concatenated_dir, os.path.splitext(os.path.basename(filename))[0] + \"_concatenated.txt\")\n",
    "    with open(concatenated_filename, \"w\", encoding='utf-8') as f:\n",
    "        f.write(remove_hebrew_diacritics(concatenated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_verses(\"Jerusalem_Talmud_complete.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process was completed without error.\n"
     ]
    }
   ],
   "source": [
    "# For all the files in the directory, browse, clean and concatenate the verses\n",
    "import os\n",
    "path = \"/\"\n",
    "\n",
    "# Variable to track whether an error has occurred\n",
    "error_occurred = False\n",
    "\n",
    "# Clean the texts in the directory\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            browse_and_clean(os.path.join(path, filename))\n",
    "        except Exception as e:\n",
    "            error_occurred = True\n",
    "            print(f\"An error occurred while processing file {filename}: {str(e)}\")\n",
    "\n",
    "# # create an index\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         try:\n",
    "#             create_index(os.path.join(\"cleaned\", filename))\n",
    "#         except Exception as e:\n",
    "#             error_occurred = True\n",
    "#             print(f\"An error occurred while creating index for file {filename}: {str(e)}\")\n",
    "\n",
    "# # Concatenate the verses in the 'cleaned' directory\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         try:\n",
    "#             concatenate_verses(os.path.join(\"cleaned\", filename))\n",
    "#         except Exception as e:\n",
    "#             error_occurred = True\n",
    "#             print(f\"An error occurred while concatenating verses for file {filename}: {str(e)}\")\n",
    "\n",
    "# Afficher un message indiquant que le processus s'est terminé sans erreur\n",
    "if not error_occurred:\n",
    "    print(\"The process was completed without error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# path = \"../../Corpuses_from_Sefaria/Liturgy\"\n",
    "\n",
    "# # Variable to track whether an error has occurred\n",
    "# error_occurred = False\n",
    "\n",
    "# # Clean the texts in the directory\n",
    "# for filename in os.listdir(path):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         browse_and_clean(os.path.join(path, filename))\n",
    "\n",
    "# # create an index\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         create_index(os.path.join(\"cleaned\", filename))\n",
    "\n",
    "# # Concatenate the verses in the 'cleaned' directory\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         concatenate_verses(os.path.join(\"cleaned\", filename))\n",
    "\n",
    "# # Afficher un message indiquant que le processus s'est terminé sans erreur\n",
    "# if not error_occurred:\n",
    "#     print(\"The process was completed without error.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
