{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation pipeline\n",
    "- Load file to prepare\n",
    "- Clean the file\n",
    "- Create an index\n",
    "- Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import unicodedata\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n",
      "2\n",
      "['Daf', 'Line']\n"
     ]
    }
   ],
   "source": [
    "# open the file Jerusalem_Talmud_complete.json\n",
    "with open('Babylon_Talmud_complete.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for book in data:\n",
    "    print(len(book['sectionNames']))\n",
    "    print(book['sectionNames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_book_structure(file):\n",
    "    '''\n",
    "    Identifies the structure of the book\n",
    "    Returns:\n",
    "    - 'book_simple' if json contains a single book with a simple structure (e.g. chapter -> verse (-> subverse if needed))\n",
    "    - 'corpus_simple' if json contains a corpus of books (like Tanakh or Talmud) with a simple structure\n",
    "    - 'book_complex' if json contains a single book with complex structure (with nodes)\n",
    "    - 'corpus_complex' if json contains a corpus of books with complex structure (with nodes)\n",
    "    More about the Sefaria text structures:\n",
    "    https://developers.sefaria.org/docs/the-index-schema\n",
    "    '''\n",
    "    # Open the json file and load the content\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Check if data is a dictionary\n",
    "    if isinstance(data, dict):\n",
    "        if 'text' in data:\n",
    "            if isinstance(data['text'], list):\n",
    "                # Check if the first element is a dictionary\n",
    "                if len(data['text']) > 0 and isinstance(data['text'][0], dict):\n",
    "                    return 'book_simple'\n",
    "                return 'book_simple'  # For simple structures\n",
    "            elif isinstance(data['text'], dict):\n",
    "                return 'book_complex'\n",
    "\n",
    "    # Check if data is a list of dictionaries\n",
    "    elif isinstance(data, list):\n",
    "        if len(data) > 0 and isinstance(data[0], dict):\n",
    "            if 'text' in data[0]:\n",
    "                if isinstance(data[0]['text'], list):\n",
    "                    # Check if the first element in 'text' is a dictionary\n",
    "                    if len(data[0]['text']) > 0 and isinstance(data[0]['text'][0], dict):\n",
    "                        return 'corpus_complex'\n",
    "                    return 'corpus_simple'\n",
    "                elif isinstance(data[0]['text'], dict):\n",
    "                    return 'corpus_complex'\n",
    "    \n",
    "    # Default return for any other structure\n",
    "    return 'unknown_structure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus_simple'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'Babylon_Talmud_complete.json'\n",
    "identify_book_structure(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_simple_book_structure_and_show_unique_chars(filename):\n",
    "    '''\n",
    "    This function reads a simple structured file containing a single book and returns a set of unique characters in the file.\n",
    "    The aim is to quickly identify characters that should be removed from the text.\n",
    "    '''\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    character_set = set()\n",
    "\n",
    "    for chapter in data[\"text\"]:\n",
    "        for verse in chapter:\n",
    "            if isinstance(verse, str):\n",
    "                character_set.update(verse)\n",
    "            elif isinstance(verse, list):\n",
    "                for sub_verse in verse:\n",
    "                    character_set.update(sub_verse)\n",
    "\n",
    "    return character_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_complex_book_structure_and_show_unique_chars(filename):\n",
    "\n",
    "    '''\n",
    "    This function reads a complex structured file containing a single book and returns a set of unique characters in the file.\n",
    "    The aim is to quickly indentify characters that should be removed from the text.\n",
    "    '''\n",
    "\n",
    "    def browse_and_show_unique_chars(nodes, level=0, character_set=None):\n",
    "        '''\n",
    "        recursively browse the nodes and return a set of unique characters in the file.\n",
    "        '''\n",
    "\n",
    "        if character_set is None:\n",
    "            character_set = set()\n",
    "\n",
    "        for value in nodes.values():\n",
    "            # If the node value is a list, add the unique characters of each list item to the set\n",
    "            if isinstance(value, list):\n",
    "                for elem in value:\n",
    "                    # If the element is a list, iterate through its items and add their unique characters to the set\n",
    "                    if isinstance(elem, list):\n",
    "                        for sub_elem in elem:\n",
    "                            if isinstance(sub_elem, list):\n",
    "                                for sub_sub_elem in sub_elem:\n",
    "                                    character_set.update(sub_sub_elem)                         \n",
    "                            else:\n",
    "                                character_set.update(sub_elem)\n",
    "                    else:\n",
    "                        character_set.update(elem)\n",
    "\n",
    "            # If the node value is a dictionary, recursively call the browse_nodes function on the dictionary.\n",
    "            elif isinstance(value, dict):\n",
    "                browse_and_show_unique_chars(value, level + 1, character_set)\n",
    "\n",
    "        return character_set\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    nodes = data['text']\n",
    "\n",
    "    return browse_and_show_unique_chars(nodes, level=0, character_set=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_simple_corpus_structure_and_show_unique_chars(filename):\n",
    "    '''\n",
    "    This function reads a simple structured file containing a corpus of books and returns a set of unique characters in the file.\n",
    "    The aim is to quickly identify characters that should be removed from the text.\n",
    "    '''\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    character_set = set()\n",
    "    for book in data:\n",
    "        for chapter in book[\"text\"]:\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    character_set.update(verse)\n",
    "                elif isinstance(verse, list):\n",
    "                    for sub_verse in verse:\n",
    "                        character_set.update(sub_verse)\n",
    "    return character_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " 'b',\n",
       " 'g',\n",
       " 'i',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " '{',\n",
       " '}',\n",
       " '\\xa0',\n",
       " 'ְ',\n",
       " 'ֱ',\n",
       " 'ֲ',\n",
       " 'ֳ',\n",
       " 'ִ',\n",
       " 'ֵ',\n",
       " 'ֶ',\n",
       " 'ַ',\n",
       " 'ָ',\n",
       " 'ֹ',\n",
       " 'ֻ',\n",
       " 'ּ',\n",
       " 'ֽ',\n",
       " 'ֿ',\n",
       " 'ׁ',\n",
       " 'ׂ',\n",
       " 'ׇ',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'ף',\n",
       " 'פ',\n",
       " 'ץ',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת',\n",
       " '׳',\n",
       " '״',\n",
       " '\\u200e',\n",
       " '–',\n",
       " '—',\n",
       " '…',\n",
       " 'הּ'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "file = 'Babylon_Talmud_complete.json'\n",
    "browse_simple_corpus_structure_and_show_unique_chars(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_file_and_show_unique_chars(file):\n",
    "    '''\n",
    "    Show unique characters in the file, depending on the structure of the file (simple or complex, book or corpus)\n",
    "    '''\n",
    "    if identify_book_structure(file) == 'book_simple':\n",
    "        return browse_simple_book_structure_and_show_unique_chars(file)\n",
    "    elif identify_book_structure(file) == 'book_complex':\n",
    "        return browse_complex_book_structure_and_show_unique_chars(file)\n",
    "    elif identify_book_structure(file) == 'corpus_simple':\n",
    "        return browse_simple_corpus_structure_and_show_unique_chars(file)\n",
    "    else:\n",
    "        return 'Unknown structure, cannot browse the file.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " '|',\n",
       " '·',\n",
       " '͏',\n",
       " '־',\n",
       " '׀',\n",
       " '׃',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'ף',\n",
       " 'פ',\n",
       " 'ץ',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת',\n",
       " 'װ',\n",
       " '׳',\n",
       " '״',\n",
       " '‘'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'cleaned/Jerusalem_Talmud_complete_clean.json'\n",
    "browse_file_and_show_unique_chars(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_simple_corpus_structure_and_show_specific_char(filename, target_character):\n",
    "    '''\n",
    "    This function reads a simple structured file containing a corpus of books and returns lines containing a specified character.\n",
    "    '''\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    lines_with_target_char = []\n",
    "    for book in data:\n",
    "        for chapter in book[\"text\"]:\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    if target_character in verse:\n",
    "                        lines_with_target_char.append(verse)\n",
    "                elif isinstance(verse, list):\n",
    "                    for sub_verse in verse:\n",
    "                        if target_character in sub_verse:\n",
    "                            lines_with_target_char.append(sub_verse)\n",
    "    \n",
    "    return lines_with_target_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['הכל מודין שאין ממון אצל מיתה דכתיב ומכה (נפש) בהמה ישלמנה ומכה אדם יומת. מה מכה בהמה לא חלקתה בה בין שוגג בין מזיד &;לפטור&; [לחייבו] < -=\"   \" -=\"\"> ממון אף מכה אדם לא תחלוק בו בין שוגג בין מזיד לפטור. במה פליגין. בממון אצל מכות. רבי יוחנן אמר אין ממון אצל מיתה ויש ממון אצל מכות. ורבי שמעון בן לקיש אמר כשם שאין ממון אצל מיתה כך אין ממון אצל מכות.',\n",
       " 'רבי &;אחא&; בשם רב. אסור לדוש על גבי זמורה בשבת. היא שורשי אילן היא קולסי אכרוב. בגבוהין שלשה. אבל אם אינן גבוהין שלשה &;כ&;ארץ הם.',\n",
       " '[שני אחים הוון בכפר חריבה. והוון רומים אזלין עליהון ומקטלין לון. ואמרין. כל־סמא דמילתא ניתי כלילא על רישיהון. אמרין. מבדקינן אוף חד זמן. מנפקין פגע ביה חד סב. אמר לון. ברייכון סעודינכון. אמר. לא יסעוד ולא [יסמוך] &;יכסוף&; הלא־אתה אלקים זנחתנו.]',\n",
       " 'על־כל המתים הוא אסור לילך בסעודה עד שלשים יום. על אביו ועל אמו עד שנים עשר חודש. אם היתה חבורת מצוה או קידוש החודש &;מותר&;.',\n",
       " '&;הרי&; שהיה מחליף בגדים כל־שבעה חייב לקרוע את כולן. רבי חייה רבה ורבי חמא אבוי דרבי הושעיה תריהון אמרין. כולהן אסורין באיחוי. בר קפרא אמר. אין לך אסור באיחוי אלא יום הראשון בלבד. אמר רבי חונה. פלגא אחרת ביניהון. מאן דמר. כולהן אסורין באיחוי. עושה שאר הימים כיום הראשון. אפילו יש עליו כמה בגדים חייב לקרוע את כולן. מאן דמר. אין לך אסור באיחוי אלא יום הראשון בלבד. עושה שאר הימים תוספת. אפילו יש עליו כמה בגדים אינו קורע אלא את העליון בלבד.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'cleaned/Jerusalem_Talmud_complete_clean.json'\n",
    "target_character = '&'\n",
    "browse_simple_corpus_structure_and_show_specific_char(filename, target_character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_simple_book(filename):\n",
    "    '''\n",
    "    Browses simple structures files and create index entries.\n",
    "    With the index, we can then quickly identify the reference of a text chunk,\n",
    "    knowing the start and end character position of the text chunk in the book.\n",
    "    '''\n",
    "    # Load the file\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create a list to store index entries\n",
    "    index = {}\n",
    "    # add the title of the book to the index\n",
    "    index[\"title\"] = data[\"title\"]\n",
    "    index['categories'] = data['categories']\n",
    "    index['sectionNames'] = data['sectionNames']\n",
    "    index['lines'] = []\n",
    "\n",
    "    # Generate a unique identifier for each of the smallest text unit,\n",
    "    # E.g. verses.\n",
    "    def generate_uid(title, number):\n",
    "        return f\"{title.replace(' ', '_')}_{number}\"\n",
    "\n",
    "    # Counts the chars, and fills the index with start and end char position\n",
    "    # Hebrew diacritics are removed from the count.\n",
    "    def browse_text_and_count_verses_chars(text):\n",
    "        def remove_hebrew_diacritics(text):\n",
    "            normalized_text = unicodedata.normalize('NFKD', text)\n",
    "            return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "        # Variable to track unique identification number\n",
    "        uid = 1\n",
    "        # Initialize character counter\n",
    "        verse_start_char = 1\n",
    "        # Intialize chapter counter\n",
    "        chapter_number = 1\n",
    "\n",
    "        for chapter in data['text']:\n",
    "            # Initialize verse counter\n",
    "            verse_number = 1\n",
    "\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    verse = remove_hebrew_diacritics(verse)\n",
    "                    verse_end_char = verse_start_char + len(verse) - 1\n",
    "\n",
    "                    # Add verse to index with UID\n",
    "                    index['lines'].append({\n",
    "                        \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                        \"chapter_number\": chapter_number,\n",
    "                        \"verse_number\": verse_number,\n",
    "                        \"start_char\": verse_start_char,\n",
    "                        \"end_char\": verse_end_char,\n",
    "                        \"length\": len(verse),\n",
    "                        \"text\": verse\n",
    "                    })\n",
    "\n",
    "                    # Increment UID for next verse\n",
    "                    uid += 1\n",
    "\n",
    "                    # Increment verse number for next verse\n",
    "                    verse_number += 1\n",
    "\n",
    "                    # Update character counter for next verse\n",
    "                    verse_start_char = verse_end_char + 1\n",
    "                elif isinstance(verse, list):\n",
    "\n",
    "                    # Initialize subverse counter\n",
    "                    sub_verse_number = 1\n",
    "\n",
    "                    for sub_verse in verse:\n",
    "                        sub_verse = remove_hebrew_diacritics(sub_verse)\n",
    "                        sub_verse_end_char = verse_start_char + len(sub_verse) - 1\n",
    "\n",
    "                        # Add verse to index with UID\n",
    "                        index['lines'].append({\n",
    "                            \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                            \"chapter_number\": chapter_number,\n",
    "                            \"verse_number\": verse_number,\n",
    "                            \"sub_verse_number\": sub_verse_number,\n",
    "                            \"start_char\": verse_start_char,\n",
    "                            \"end_char\": sub_verse_end_char,\n",
    "                            \"length\": len(sub_verse),\n",
    "                            \"text\": sub_verse\n",
    "                        })\n",
    "\n",
    "                        # Increment UID for next verse\n",
    "                        uid += 1\n",
    "\n",
    "                        # Increment verse number for next verse\n",
    "                        sub_verse_number += 1\n",
    "\n",
    "                        # Update character counter for next verse\n",
    "                        verse_start_char = sub_verse_end_char + 1\n",
    "                    verse_number += 1\n",
    "            chapter_number += 1\n",
    "    \n",
    "    # Load the nodes\n",
    "    text = data['text']\n",
    "    browse_text_and_count_verses_chars(text)\n",
    "\n",
    "    # Create a new directory named \"indexes\" if it doesn't exist\n",
    "    index_dir = \"indexes\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    # Save the index to a file, in the \"indexes\" directory\n",
    "    title = index[\"title\"]\n",
    "    index_filename = os.path.join(index_dir, \"index_\" + title.replace(' ', '_') + \".json\")\n",
    "    with open(index_filename, \"w\") as f:\n",
    "        json.dump(index, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_from_complex_book(filename):\n",
    "    '''\n",
    "    Browses complex structured files and create index entries.\n",
    "    With the index, we can then quickly identify the reference of a text chunk,\n",
    "    knowing the start and end character position of the text chunk in the book.\n",
    "    '''\n",
    "\n",
    "    # Load the file\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create a list to store index entries\n",
    "    index = {}\n",
    "    # add the title of the book to the index\n",
    "    index[\"title\"] = data[\"title\"]\n",
    "    index['categories'] = data['categories']\n",
    "    index['lines'] = []\n",
    "\n",
    "    def generate_uid(title, number):\n",
    "        return f\"{title.replace(' ', '_')}_{number}\"\n",
    "\n",
    "    def browse_nodes_and_count_verses_chars(nodes, parent_keys=[], level=0):\n",
    "        def remove_hebrew_diacritics(text):\n",
    "            normalized_text = unicodedata.normalize('NFKD', text)\n",
    "            return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "\n",
    "        # Variable to track unique identification number\n",
    "        uid = 1\n",
    "\n",
    "        # Initialize character counter\n",
    "        verse_start_char = 1\n",
    "\n",
    "        item_lvl_1_number, item_lvl_2_number, item_lvl_3_number = 0, 0, 0\n",
    "\n",
    "\n",
    "        for key, value in nodes.items():\n",
    "            # Add the current key to the list of parent keys\n",
    "            current_keys = parent_keys + [key]\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                # If the node value is a list, check if it contains strings or lists\n",
    "\n",
    "                # Initialize verse counter\n",
    "                item_lvl_1_number = 1\n",
    "\n",
    "                for item_lvl_1 in value:\n",
    "                # 1st lvl: If the item is a string, treat it as a single verse\n",
    "                    if isinstance(item_lvl_1, str):\n",
    "                        # remove hebrew vowels\n",
    "                        item_lvl_1 = remove_hebrew_diacritics(item_lvl_1)\n",
    "                        verse_end_char = verse_start_char + len(item_lvl_1) - 1\n",
    "\n",
    "                        # Add verse to index with UID\n",
    "                        index['lines'].append({\n",
    "                            \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                            \"parent_titles\": current_keys,\n",
    "                            \"item_lvl_1_number\": item_lvl_1_number,\n",
    "                            \"start_char\": verse_start_char,\n",
    "                            \"end_char\": verse_end_char,\n",
    "                            \"length\": len(item_lvl_1),\n",
    "                            \"text\": item_lvl_1\n",
    "                        })\n",
    "\n",
    "                        # Increment UID for next verse\n",
    "                        uid += 1\n",
    "\n",
    "\n",
    "                        # Update character counter for next verse\n",
    "                        verse_start_char = verse_end_char + 1\n",
    "\n",
    "\n",
    "                    # 1st lvl: if the item in value is a list, treat it as a list\n",
    "                    if isinstance(item_lvl_1, list):\n",
    "                        # Initialize verse counter\n",
    "                        item_lvl_2_number = 1\n",
    "                        for item_lvl_2 in item_lvl_1:\n",
    "                            # 2nd lvl:\n",
    "                            #  if item is a string, treat it as a single verse\n",
    "                            if isinstance(item_lvl_2, str):\n",
    "\n",
    "                                # remove hebrew vowels\n",
    "                                item_lvl_2 = remove_hebrew_diacritics(item_lvl_2)\n",
    "                                verse_end_char = verse_start_char + len(item_lvl_2) - 1\n",
    "\n",
    "                                # Add verse to index with UID\n",
    "                                index['lines'].append({\n",
    "                                    \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                                    \"parent_titles\": current_keys,\n",
    "                                    \"item_lvl_1_number\": item_lvl_1_number,\n",
    "                                    \"item_lvl_2_number\": item_lvl_2_number,\n",
    "                                    \"start_char\": verse_start_char,\n",
    "                                    \"end_char\": verse_end_char,\n",
    "                                    \"length\": len(item_lvl_2),\n",
    "                                    \"text\": item_lvl_2\n",
    "                                })\n",
    "\n",
    "                                # Increment UID for next verse\n",
    "                                uid += 1\n",
    "\n",
    "                                # Increment verse number for next verse\n",
    "                                item_lvl_2_number += 1\n",
    "\n",
    "                                # Update character counter for next verse\n",
    "                                verse_start_char = verse_end_char + 1\n",
    "\n",
    "                            # 2nd lvl: if the item is a list\n",
    "                            elif isinstance(item_lvl_2, list):\n",
    "                                # Initialize subverse counter\n",
    "                                item_lvl_3_number = 1\n",
    "                                for item_lvl_3 in item_lvl_2:\n",
    "                                \n",
    "                                    # 3rd lvl: if the sub_verse is a string\n",
    "                                    if isinstance(item_lvl_3, str):\n",
    "                                        item_lvl_3 = remove_hebrew_diacritics(item_lvl_3)\n",
    "                                        verse_end_char = verse_start_char + len(item_lvl_3) - 1\n",
    "\n",
    "                                        # Add verse to index with UID\n",
    "                                        index['lines'].append({\n",
    "                                        \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                                        \"parent_titles\": current_keys,\n",
    "                                        \"item_lvl_1_number\": item_lvl_1_number,\n",
    "                                        \"item_lvl_2_number\": item_lvl_2_number,\n",
    "                                        \"item_lvl_3_number\": item_lvl_3_number,\n",
    "                                        \"start_char\": verse_start_char,\n",
    "                                        \"end_char\": verse_end_char,\n",
    "                                        \"length\": len(item_lvl_3),\n",
    "                                        \"text\": item_lvl_3\n",
    "                                        })\n",
    "\n",
    "                                        # Increment UID for next verse\n",
    "                                        uid += 1\n",
    "\n",
    "                                        # Increment verse number for next verse\n",
    "                                        item_lvl_3_number += 1\n",
    "\n",
    "                                        # Update character counter for next verse\n",
    "                                        verse_start_char = verse_end_char + 1\n",
    "\n",
    "                                    # 3rd lvl: if the sub_verse is a list\n",
    "                                    if isinstance(item_lvl_3, list):\n",
    "                                        print(\"nested verses detected and not handled !\")\n",
    "\n",
    "                                item_lvl_2_number += 1\n",
    "                        # Increment verse number for next verse\n",
    "                        item_lvl_1_number += 1\n",
    "\n",
    "\n",
    "            # If the node value is a dictionary, recursively call the browse_nodes_and_count_verses_chars function on the dictionary.\n",
    "            elif isinstance(value, dict):\n",
    "                browse_nodes_and_count_verses_chars(value, parent_keys=current_keys, level=level + 1)\n",
    "\n",
    "    # Load the nodes\n",
    "    nodes = data['text']\n",
    "    browse_nodes_and_count_verses_chars(nodes, level=0)\n",
    "\n",
    "    # Create a new directory named \"indexes\" if it doesn't exist\n",
    "    index_dir = \"indexes\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    # Save the index to a file, in the \"indexes\" directory\n",
    "    title = index[\"title\"]\n",
    "    index_filename = os.path.join(index_dir, \"index_\" + title.replace(' ', '_') + \".json\")\n",
    "    with open(index_filename, \"w\") as f:\n",
    "        json.dump(index, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_names_in_simple_corpus(data):\n",
    "    '''\n",
    "    This function returns the number of sections and the section names in a simple structured corpus.\n",
    "    If the section number is not the same for all books:\n",
    "    - print(\"All books do not have the same number of sections.\")\n",
    "    - print the max number of sections and the section names for the book with the max number of sections.\n",
    "    '''    \n",
    "\n",
    "    # Initialize variables to track section information\n",
    "    section_counts = []\n",
    "    section_names = []\n",
    "\n",
    "    for book in data:\n",
    "        # Get the number of sections and section names for each book\n",
    "        number_of_sections = len(book['sectionNames'])\n",
    "        section_counts.append(number_of_sections)\n",
    "        section_names.append(book['sectionNames'])\n",
    "\n",
    "    # Check if the number of sections is consistent across all books\n",
    "    if len(set(section_counts)) == 1:\n",
    "        # If the number of sections is the same for all books\n",
    "        num_sections = section_counts[0]  # Number of sections\n",
    "        names = section_names[0]  # Section names\n",
    "        return num_sections, names\n",
    "    else:\n",
    "        # If the number of sections is different for some books\n",
    "        print(\"All books do not have the same number of sections.\")\n",
    "        max_sections = max(section_counts)\n",
    "        max_index = section_counts.index(max_sections)\n",
    "        max_names = section_names[max_index]\n",
    "        print(f\"Maximum number of sections: {max_sections}\")\n",
    "        print(\"Section names for the book with the maximum number of sections:\")\n",
    "        for name in max_names:\n",
    "            print(name)\n",
    "        return (max_sections, max_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, ['Daf', 'Line'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'Babylon_Talmud_complete.json'\n",
    "with open(file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "get_section_names_in_simple_corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned/Babylon_Talmud_complete_clean.json not found.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def create_index_from_simple_corpus(filename):\n",
    "    '''\n",
    "    Browses a simple structured corpus and create index entries.\n",
    "    With the index, we can then quickly identify the reference of a text chunk,\n",
    "    knowing the start and end character position of the text chunk in the book.\n",
    "    '''\n",
    "    # Load the file\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File {filename} not found.\")\n",
    "        return\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Assuming the function get_section_names_in_simple_corpus exists and is defined elsewhere\n",
    "    nb_sections, section_names = get_section_names_in_simple_corpus(data)\n",
    "\n",
    "    # Create a list to store index entries\n",
    "    index = {}\n",
    "\n",
    "    # Add the title of the corpus to the index\n",
    "    index['title'] = os.path.basename(filename).split('.')[0]\n",
    "    index['categories'] = data[0]['categories'][:2]\n",
    "    index['sectionNames'] = section_names\n",
    "    index['lines'] = []\n",
    "\n",
    "    # Generate a unique identifier for each of the smallest text units\n",
    "    def generate_uid(title, number):\n",
    "        return f\"{title.replace(' ', '_')}_{number}\"\n",
    "\n",
    "    # Variable to track unique identification number\n",
    "    uid = 1\n",
    "\n",
    "    for book in data:\n",
    "        # Initialize character counter\n",
    "        verse_start_char = 1\n",
    "        # Initialize chapter counter\n",
    "        chapter_number = 1\n",
    "\n",
    "        for chapter in book['text']:\n",
    "            # Initialize verse counter\n",
    "            verse_number = 1\n",
    "\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    verse_end_char = verse_start_char + len(verse) - 1\n",
    "\n",
    "                    # Add verse to index with UID\n",
    "                    index['lines'].append({\n",
    "                        \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                        \"book_title\": book[\"title\"],\n",
    "                        \"chapter_number\": chapter_number,\n",
    "                        \"verse_number\": verse_number,\n",
    "                        \"start_char\": verse_start_char,\n",
    "                        \"end_char\": verse_end_char,\n",
    "                        \"length\": len(verse),\n",
    "                        \"text\": verse\n",
    "                    })\n",
    "\n",
    "                    # Increment UID for next verse\n",
    "                    uid += 1\n",
    "\n",
    "                    # Increment verse number for next verse\n",
    "                    verse_number += 1\n",
    "\n",
    "                    # Update character counter for next verse\n",
    "                    verse_start_char = verse_end_char + 1\n",
    "                elif isinstance(verse, list):\n",
    "                    # Initialize subverse counter\n",
    "                    sub_verse_number = 1\n",
    "\n",
    "                    for sub_verse in verse:\n",
    "                        sub_verse_end_char = verse_start_char + len(sub_verse) - 1\n",
    "\n",
    "                        # Add verse to index with UID\n",
    "                        index['lines'].append({\n",
    "                            \"uid\": generate_uid(index['title'], uid),  # UID unique\n",
    "                            \"book_title\": book[\"title\"],\n",
    "                            \"chapter_number\": chapter_number,\n",
    "                            \"verse_number\": verse_number,\n",
    "                            \"sub_verse_number\": sub_verse_number,\n",
    "                            \"start_char\": verse_start_char,\n",
    "                            \"end_char\": sub_verse_end_char,\n",
    "                            \"length\": len(sub_verse),\n",
    "                            \"text\": sub_verse\n",
    "                        })\n",
    "\n",
    "                        # Increment UID for next verse\n",
    "                        uid += 1\n",
    "\n",
    "                        # Increment subverse number for next subverse\n",
    "                        sub_verse_number += 1\n",
    "\n",
    "                        # Update character counter for next subverse\n",
    "                        verse_start_char = sub_verse_end_char + 1\n",
    "                    verse_number += 1\n",
    "            chapter_number += 1    \n",
    "\n",
    "    # Create a new directory named \"indexes\" if it doesn't exist\n",
    "    index_dir = \"indexes\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    # Save the index to a file, in the \"indexes\" directory\n",
    "    title = index[\"title\"]\n",
    "    index_filename = os.path.join(index_dir, \"index_\" + title.replace(' ', '_') + \".json\")\n",
    "    with open(index_filename, \"w\") as f:\n",
    "        json.dump(index, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# File to process\n",
    "file = 'cleaned/Babylon_Talmud_complete_clean.json'\n",
    "\n",
    "# Create the index\n",
    "create_index_from_simple_corpus(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_and_clean(filename):\n",
    "\n",
    "    '''\n",
    "    Remove unwanted characters from the text (e.g. numbers, special unicodes, html tags, etc.)\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "    def remove_latin_characters(text):\n",
    "        return re.sub(r'[a-zA-Z]', '', text)\n",
    "\n",
    "    def remove_hebrew_diacritics(text):\n",
    "        normalized_text = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
    "    \n",
    "    def remove_angle_brackets(text):\n",
    "        return re.sub(r'\\<.*?\\>', '', text)\n",
    "    \n",
    "    def remove_latin_punctuation(text):\n",
    "        chars_to_remove = {'&', '?', '*', '='}\n",
    "        for char in chars_to_remove:\n",
    "            text = text.replace(char, '')\n",
    "        return text\n",
    "\n",
    "    def remove_special_unicodes(text):\n",
    "        chars_to_remove = {'\\n', '\\u2003', '\\u200d', '\\u200e', '\\u202c','\\u2009', '\\xa0' ,'\\xad', '\\u200c', '\\u200f'}\n",
    "        for char in chars_to_remove:\n",
    "            text = text.replace(char, '')\n",
    "        return text\n",
    "\n",
    "    def remove_numbers(text):\n",
    "        return ''.join(char for char in text if not char.isdigit())\n",
    "    \n",
    "    def clean_html(text):\n",
    "        if text.strip():  # Checks if text is not empty after removing spaces\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            \n",
    "            # Specifically deletes <b> and </b> tags\n",
    "            for bold_tag in soup.find_all(['b', 'strong']):\n",
    "                bold_tag.unwrap()\n",
    "            \n",
    "            return soup.get_text()\n",
    "        else:\n",
    "            return text  # Returns text unchanged if empty\n",
    "\n",
    "    # we try to catch and display the MarkupResemblesLocatorWarning warning\n",
    "    # import warnings\n",
    "\n",
    "    # def clean_html(text):\n",
    "    #     with warnings.catch_warnings(record=True) as w:\n",
    "    #         warnings.simplefilter(\"always\")  # Capture tous les avertissements\n",
    "    #         if text.strip():\n",
    "    #             text_to_parse = text  # Stocker le texte avant de l'analyser\n",
    "    #             soup = BeautifulSoup(text, 'html.parser')\n",
    "    #             text_without_html = soup.get_text()\n",
    "    #             # Parcourir tous les avertissements capturés\n",
    "    #             for warning in w:\n",
    "    #                 if \"MarkupResemblesLocatorWarning\" in str(warning.message):\n",
    "    #                     print(\"MarkupResemblesLocatorWarning captured while parsing the following text:\")\n",
    "    #                     print(text_to_parse)\n",
    "    #             return text\n",
    "    #         else:\n",
    "    #             return text\n",
    "  \n",
    "    \n",
    "    def remove_braces(text):\n",
    "        return re.sub(r'\\{.*?\\}', '', text)\n",
    "   \n",
    "\n",
    "    def clean_text(text):\n",
    "        \n",
    "        text = remove_latin_characters(text)\n",
    "        text = remove_latin_punctuation(text)\n",
    "        text = remove_hebrew_diacritics(text)\n",
    "        text = clean_html(text)\n",
    "        text = remove_special_unicodes(text)\n",
    "        text = remove_numbers(text)\n",
    "        text = remove_braces(text)\n",
    "        text = remove_angle_brackets(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text_structure = identify_book_structure(filename)\n",
    "\n",
    "    if text_structure == 'corpus_simple':\n",
    "        for book in data:\n",
    "            for chapter in book[\"text\"]:\n",
    "                for i, verse in enumerate(chapter):\n",
    "                    if isinstance(verse, str):\n",
    "                        chapter[i] = clean_text(verse)\n",
    "                    elif isinstance(verse, list):\n",
    "                        for j, sub_verse in enumerate(verse):\n",
    "                            verse[j] = clean_text(sub_verse)\n",
    "\n",
    "    elif isinstance(data['text'], list):\n",
    "\n",
    "        # Browse and clean verses directly in the data structure\n",
    "        for chapter in data[\"text\"]:\n",
    "            if isinstance(chapter, str):\n",
    "                chapter = clean_text(chapter)\n",
    "            else:\n",
    "                for i, verse in enumerate(chapter):\n",
    "                    if isinstance(verse, str):\n",
    "                        chapter[i] = clean_text(verse)\n",
    "                    elif isinstance(verse, list):\n",
    "                        for j, sub_verse in enumerate(verse):\n",
    "                            verse[j] = clean_text(sub_verse)        \n",
    "    \n",
    "    \n",
    "    elif isinstance(data['text'], dict):\n",
    "        \n",
    "        def browse_clean_complex(nodes, level=0):\n",
    "            for key, value in nodes.items():\n",
    "                if isinstance(value, list):\n",
    "                    for i, elem in enumerate(value):\n",
    "                        if isinstance(elem, list):\n",
    "                            for j, sub_elem in enumerate(elem):\n",
    "                                if isinstance(sub_elem, list):\n",
    "                                    for k, sub_sub_elem in enumerate(sub_elem):\n",
    "                                        nodes[key][i][j][k] = clean_text(sub_sub_elem)\n",
    "                                else:\n",
    "                                    nodes[key][i][j] = clean_text(sub_elem)\n",
    "                        else:\n",
    "                            nodes[key][i] = clean_text(elem)\n",
    "                elif isinstance(value, dict):\n",
    "                    browse_clean_complex(value, level + 1)\n",
    "            return nodes\n",
    "\n",
    "        \n",
    "        # Load the nodes\n",
    "        nodes = data['text']\n",
    "        browse_clean_complex(nodes, level=0)\n",
    "\n",
    "    # Create a new directory named \"cleaned\" if it doesn't exist\n",
    "    if not os.path.exists('cleaned'):\n",
    "        os.makedirs('cleaned')\n",
    "\n",
    "    # Define the cleaned filename with spaces replaced by underscores\n",
    "    base_filename = os.path.basename(filename)\n",
    "    cleaned_filename = os.path.join('cleaned', base_filename.replace(' ', '_').split('.')[0] + \"_clean.json\")\n",
    "    \n",
    "    \n",
    "    # Save the cleaned file in the \"cleaned\" directory\n",
    "    with open(cleaned_filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156758/3242838570.py:34: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "browse_and_clean('Babylon_Talmud_complete.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_and_show_unique_chars(filename):\n",
    "    '''\n",
    "    Show unique characters in the file, depending on the structure of the file (simple or complex, book or corpus)\n",
    "    '''\n",
    "    if identify_book_structure(filename) == 'book_simple':\n",
    "        return browse_simple_book_structure_and_show_unique_chars(filename)\n",
    "    elif identify_book_structure(filename) == 'book_complex':\n",
    "        return browse_complex_book_structure_and_show_unique_chars(filename)\n",
    "    elif identify_book_structure(filename) == 'corpus_simple':\n",
    "        return browse_simple_corpus_structure_and_show_unique_chars(filename)\n",
    "    else:\n",
    "        return 'Unknown structure, cannot browse the file.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "] : 93\n",
      ". : 46\n",
      ") : 41\n",
      "׃ : 1475\n",
      "ם : 1501\n",
      "ך : 1498\n",
      "פ : 1508\n",
      "צ : 1510\n",
      "· : 183\n",
      "ט : 1496\n",
      "נ : 1504\n",
      "\" : 34\n",
      "ר : 1512\n",
      "׀ : 1472\n",
      "ש : 1513\n",
      "ג : 1490\n",
      "‘ : 8216\n",
      "ל : 1500\n",
      "/ : 47\n",
      "- : 45\n",
      "ו : 1493\n",
      ": : 58\n",
      "' : 39\n",
      "ת : 1514\n",
      "ק : 1511\n",
      ", : 44\n",
      "ץ : 1509\n",
      "ס : 1505\n",
      "ח : 1495\n",
      "״ : 1524\n",
      "( : 40\n",
      "ע : 1506\n",
      "מ : 1502\n",
      "[ : 91\n",
      "ה : 1492\n",
      "־ : 1470\n",
      "ן : 1503\n",
      "׳ : 1523\n",
      "װ : 1520\n",
      "; : 59\n",
      "י : 1497\n",
      "ף : 1507\n",
      "͏ : 847\n",
      "| : 124\n",
      "  : 32\n",
      "ז : 1494\n",
      "כ : 1499\n",
      "ב : 1489\n",
      "ד : 1491\n",
      "א : 1488\n"
     ]
    }
   ],
   "source": [
    "unique_chars = browse_file_and_show_unique_chars('cleaned/Babylon_Talmud_complete_clean.json')\n",
    "for c in unique_chars:\n",
    "    print(f\"{c} : {ord(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_verses(filename):\n",
    "    '''\n",
    "    Concatenate the verses of a book into a single text file.\n",
    "    Handles both simple and complex structured files.\n",
    "    '''\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    concatenated_text = \"\"\n",
    "\n",
    "    text_structure = identify_book_structure(filename)\n",
    "\n",
    "    if text_structure == 'corpus_simple':\n",
    "        for book in data:\n",
    "            for chapter in book[\"text\"]:\n",
    "                for verse in chapter:\n",
    "                    if isinstance(verse, str):\n",
    "                        concatenated_text += verse\n",
    "                    elif isinstance(verse, list):\n",
    "                        for sub_verse in verse:\n",
    "                            concatenated_text += sub_verse\n",
    "\n",
    "    elif isinstance(data['text'], list):\n",
    "        # Browse and clean verses directly in the data structure\n",
    "        for chapter in data[\"text\"]:\n",
    "            for verse in chapter:\n",
    "                if isinstance(verse, str):\n",
    "                    concatenated_text += verse\n",
    "                elif isinstance(verse, list):\n",
    "                    for sub_verse in verse:\n",
    "                        if isinstance(sub_verse, str):\n",
    "                            concatenated_text += sub_verse\n",
    "\n",
    "    elif isinstance(data['text'], dict):\n",
    "\n",
    "        def browse_concat_complex(nodes):\n",
    "            text = \"\"\n",
    "            for value in nodes.values():\n",
    "                if isinstance(value, list):\n",
    "                    for elem in value:\n",
    "                        if isinstance(elem, str):\n",
    "                            text += elem\n",
    "                        elif isinstance(elem, list):\n",
    "                            for sub_elem in elem:\n",
    "                                if isinstance(sub_elem, str):\n",
    "                                    text += sub_elem\n",
    "                                elif isinstance(sub_elem, list):\n",
    "                                    for sub_sub_elem in sub_elem:\n",
    "                                        if isinstance(sub_sub_elem, str):\n",
    "                                            text += sub_sub_elem\n",
    "                                        elif isinstance(sub_sub_elem, list):\n",
    "                                            print(\"Nested verses detected and not handled!\")\n",
    "                elif isinstance(value, dict):\n",
    "                    text += browse_concat_complex(value)\n",
    "            return text\n",
    "\n",
    "        # Load the nodes\n",
    "        concatenated_text = browse_concat_complex(data['text'])\n",
    "    \n",
    "    else:\n",
    "        print(\"Unknown structure, cannot concatenate the file.\")\n",
    "\n",
    "    # Create a new directory named \"concatenated\" if it doesn't exist\n",
    "    concatenated_dir = 'concatenated'\n",
    "    if not os.path.exists(concatenated_dir):\n",
    "        os.makedirs(concatenated_dir)\n",
    "\n",
    "    # Save the concatenated text to a file, in the \"concatenated\" directory\n",
    "    concatenated_filename = os.path.join(concatenated_dir, os.path.splitext(os.path.basename(filename))[0] + \"_concatenated.txt\")\n",
    "    with open(concatenated_filename, \"w\") as f:\n",
    "        f.write(concatenated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'cleaned/Jerusalem_Talmud_complete_clean.json'\n",
    "concatenate_verses(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process was completed without error.\n"
     ]
    }
   ],
   "source": [
    "# For all the files in the directory, browse, clean and concatenate the verses\n",
    "import os\n",
    "path = \"../../Corpuses_from_Sefaria/Bavli\"\n",
    "\n",
    "# Variable to track whether an error has occurred\n",
    "error_occurred = False\n",
    "\n",
    "# Clean the texts in the directory\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            browse_and_clean(os.path.join(path, filename))\n",
    "        except Exception as e:\n",
    "            error_occurred = True\n",
    "            print(f\"An error occurred while processing file {filename}: {str(e)}\")\n",
    "\n",
    "# create an index\n",
    "for filename in os.listdir(\"cleaned\"):\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            create_index(os.path.join(\"cleaned\", filename))\n",
    "        except Exception as e:\n",
    "            error_occurred = True\n",
    "            print(f\"An error occurred while creating index for file {filename}: {str(e)}\")\n",
    "\n",
    "# Concatenate the verses in the 'cleaned' directory\n",
    "for filename in os.listdir(\"cleaned\"):\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            concatenate_verses(os.path.join(\"cleaned\", filename))\n",
    "        except Exception as e:\n",
    "            error_occurred = True\n",
    "            print(f\"An error occurred while concatenating verses for file {filename}: {str(e)}\")\n",
    "\n",
    "# Afficher un message indiquant que le processus s'est terminé sans erreur\n",
    "if not error_occurred:\n",
    "    print(\"The process was completed without error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# path = \"../../Corpuses_from_Sefaria/Liturgy\"\n",
    "\n",
    "# # Variable to track whether an error has occurred\n",
    "# error_occurred = False\n",
    "\n",
    "# # Clean the texts in the directory\n",
    "# for filename in os.listdir(path):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         browse_and_clean(os.path.join(path, filename))\n",
    "\n",
    "# # create an index\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         create_index(os.path.join(\"cleaned\", filename))\n",
    "\n",
    "# # Concatenate the verses in the 'cleaned' directory\n",
    "# for filename in os.listdir(\"cleaned\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         concatenate_verses(os.path.join(\"cleaned\", filename))\n",
    "\n",
    "# # Afficher un message indiquant que le processus s'est terminé sans erreur\n",
    "# if not error_occurred:\n",
    "#     print(\"The process was completed without error.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
